{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of the Monte Carlo method is that we can sample our population multiple times, estimate our target and then look at the variance between those estimations. This will give us a clearer answer than just picking a number of darts and hoping that sample size is effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Monte Carlo in a Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this in Python, we can use a loop to run these simulations multiple times and gather the results. We can take the code we wrote last week and add some steps to run these calculations multiple times. Here, we've moved the $\\pi$ estimation to a function `estimate_pi` and added a loop in `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "    \n",
    "def throw_darts(n):\n",
    "    darts = 2*np.random.random((n,2)) - 1\n",
    "    return darts\n",
    "\n",
    "def in_unit_circle(p):\n",
    "    return np.linalg.norm(p,axis=1)<1\n",
    "\n",
    "def estimate_pi(n):\n",
    "    d_arr = throw_darts(n)\n",
    "    h_arr = in_unit_circle(d_arr)\n",
    "    return 4 * np.sum(h_arr) / n\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_ests = 5\n",
    "    n = 10000\n",
    "    N = n_ests*n\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(n_ests):\n",
    "        pi_ests = []\n",
    "        pi_ests.append(estimate_pi(n))\n",
    "    \n",
    "    \n",
    "    \n",
    "    pi_est_mean = np.mean(pi_ests)\n",
    "    pi_est_std  = np.std(pi_ests)\n",
    "    formstr = \"pi_est_mean = {:2.10f}, pi_est_std = {:2.10f}, n = {:d}\"\n",
    "    print(formstr.format(pi_est_mean, pi_est_std, n))\n",
    "    plt.hist(pi_ests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the code above into spyder and save it as `montecarlo_loop.py`. Run it to see what kind of result you get. Is the estimation good? Does the histagram give you any inisght?\n",
    "\n",
    "\n",
    "**Run it again, but change `n_ests` to 5000**\n",
    "\n",
    "\n",
    "How does the distribution change? Does this appear to be a more reliable calculation than with five runs? \n",
    "\n",
    "Did it take a long time to run? How long is too long? With a larger calculation, you can see how long this might take to get a good result. This is why we use parallel computing and large systems like Stampede2 to run calculations.\n",
    "\n",
    "\n",
    "How can we test whether more darts or more runs are more effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Runs vs. Darts\n",
    "\n",
    "First we'll look at changing the number of darts and keeping the number of runs the same `n_ests=50`. \n",
    "\n",
    "We should look at the standard deviation of the results as well as the histogram to see the convergence of the values.\n",
    "\n",
    "You can copy this code and paste it into a new file in Spyder, I called it `montepi-dartloop.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "    \n",
    "def throw_darts(n):\n",
    "    darts = 2*np.random.random((n,2)) - 1\n",
    "    return darts\n",
    "\n",
    "def in_unit_circle(p):\n",
    "    return np.linalg.norm(p,axis=1)<1\n",
    "\n",
    "def estimate_pi(n):\n",
    "    d_arr = throw_darts(n)\n",
    "    h_arr = in_unit_circle(d_arr)\n",
    "    return 4 * np.sum(h_arr) / n\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_ests = 50\n",
    "    darts = 100\n",
    "  #  N = n_ests*n\n",
    "    \n",
    "    \n",
    "    \n",
    "    while darts < 10000000:\n",
    "        pi_ests = []\n",
    "        start = time.time()\n",
    "        for i in range(n_ests):\n",
    "            \n",
    "        \n",
    "            pi_ests.append(estimate_pi(darts))\n",
    "        t = time.time() - start\n",
    "        print(\"Number of runs = {:d}, t = {:2.4f}\".format(n_ests, t))\n",
    "        plt.figure()\n",
    "        plt.hist(pi_ests)\n",
    "    \n",
    "        pi_est_mean = np.mean(pi_ests)\n",
    "        pi_est_std  = np.std(pi_ests)\n",
    "        formstr = \"pi_est_mean = {:2.10f}, pi_est_std = {:2.10f}, darts = {:d}\"\n",
    "        print(formstr.format(pi_est_mean, pi_est_std, darts))\n",
    "        \n",
    "        darts = darts*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output from the code, including the time for each run. Look at the code and see if you understand how all the pieces work. How does the time change as you increase the number of darts? Does the standard deviation improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of runs = 50, t = 0.0019\n",
    "pi_est_mean = 3.1536000000, pi_est_std = 0.1640214620, darts = 100\n",
    "Number of runs = 50, t = 0.0055\n",
    "pi_est_mean = 3.1442400000, pi_est_std = 0.0506622384, darts = 1000\n",
    "Number of runs = 50, t = 0.0231\n",
    "pi_est_mean = 3.1444720000, pi_est_std = 0.0173927231, darts = 10000\n",
    "Number of runs = 50, t = 0.2262\n",
    "pi_est_mean = 3.1417144000, pi_est_std = 0.0042169563, darts = 100000\n",
    "Number of runs = 50, t = 2.8439\n",
    "pi_est_mean = 3.1414833600, pi_est_std = 0.0016668142, darts = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of Darts | Result  |\n",
    "|----------------|---------|\n",
    "|  100            | <img src=\"images/100_darts.png\" width=\"200\"/> |\n",
    "|  1,000          | <img src=\"images/1000_darts.png\" width=\"200\"/> |\n",
    "|  10,000         | <img src=\"images/10000_darts.png\" width=\"200\"/> |\n",
    "|  100,000        | <img src=\"images/100000_darts.png\" width=\"200\"/> |\n",
    "|  1,000,000      | <img src=\"images/1000000_darts.png\" width=\"200\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the effect of using a large number of darts on the variance of the results. All the histograms have the same x range, 3.0 to 3.3, to make the easy to compare.\n",
    "\n",
    "\n",
    "The following program implements a loop that runs the simulation with the same number of darts `darts = 10000` and changes the number of runs from 50 to 50000. You can copy and paste this code into a new file in Spyder, I called it `montepi-loop.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "    \n",
    "def throw_darts(n):\n",
    "    darts = 2*np.random.random((n,2)) - 1\n",
    "    return darts\n",
    "\n",
    "def in_unit_circle(p):\n",
    "    return np.linalg.norm(p,axis=1)<1\n",
    "\n",
    "def estimate_pi(n):\n",
    "    d_arr = throw_darts(n)\n",
    "    h_arr = in_unit_circle(d_arr)\n",
    "    return 4 * np.sum(h_arr) / n\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_ests = 50\n",
    "    darts = 10000\n",
    "    \n",
    "   \n",
    "    while n_ests < 100000:\n",
    "        pi_ests = []\n",
    "        start = time.time()\n",
    "        for i in range(n_ests):\n",
    "            \n",
    "        \n",
    "            pi_ests.append(estimate_pi(darts))\n",
    "        t = time.time() - start\n",
    "        print(\"Number of runs = {:d}, t = {:2.4f}\".format(n_ests, t))\n",
    "        plt.figure()\n",
    "        plt.xlim((3.1,3.2))\n",
    "        plt.hist(pi_ests)\n",
    "    \n",
    "        pi_est_mean = np.mean(pi_ests)\n",
    "        pi_est_std  = np.std(pi_ests)\n",
    "        formstr = \"pi_est_mean = {:2.10f}, pi_est_std = {:2.10f}, darts = {:d}\"\n",
    "        print(formstr.format(pi_est_mean, pi_est_std, darts))\n",
    "        \n",
    "        n_ests = int(n_ests*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output from this code, you can see the time increase as you go from 50 runs to 50,000. The histograms look very different from the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of runs = 50, t = 0.0301\n",
    "pi_est_mean = 3.1447520000, pi_est_std = 0.0144816883, darts = 10000\n",
    "Number of runs = 500, t = 0.2138\n",
    "pi_est_mean = 3.1417456000, pi_est_std = 0.0169036209, darts = 10000\n",
    "Number of runs = 5000, t = 2.0944\n",
    "pi_est_mean = 3.1413668000, pi_est_std = 0.0165346623, darts = 10000\n",
    "Number of runs = 50000, t = 21.3021\n",
    "pi_est_mean = 3.1416676240, pi_est_std = 0.0164531550, darts = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of Runs | Result  |\n",
    "|----------------|---------|\n",
    "|  50            | <img src=\"images/50_runs.png\" width=\"200\"/>|\n",
    "|  500           | <img src=\"images/500_runs.png\" width=\"200\"/>|\n",
    "|  5000          | <img src=\"images/5000_runs.png\" width=\"200\"/>|\n",
    "|  50000         | <img src=\"images/50000_runs.png\" width=\"200\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Parallel?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/threads.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the image above shows, if we can break up the tasks in our code to run on separate threads at the same time, we can potentially make the code run faster. This can be difficult to get to work and it isn't always better to run this way but for some jobs, we can significantly speed up our code and also expand onto large parallel systems, like supercomputers, to calculate very large systems.\n",
    "\n",
    "This is why Monte Carlo calcuations are very popular in supercomputing scale applications, because it is fairly easy to scale them up and the individual processes are **independent** so they can run at the same time without dependencies.\n",
    "\n",
    "<img src=\"images/montecarlo_slide.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4Py\n",
    "\n",
    "The tool we will use for parallel processing is MPI4Py which is an implementation of MPI in Python. MPI stands for Message Passing Interface and is basically a way to get multiple processing units, in this case cores, to share the workload and communicate with each other. The processors are given \"ranks\". The rank 0 processor is the master processor and sends and receives information from each of the worker processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = MPI.COMM_WORLD  # sets up communication between the processors\n",
    "size = comm.Get_size() # gives number of ranks in comm\n",
    "rank = comm.Get_rank() # identifies ranks of the processors, starting with 0\n",
    "comm.scatter(data) # break up an array and send data out to all processors\n",
    "comm.gather(data) # gather data from processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MPI_scatter.png\" width=\"600\"/>\n",
    "\n",
    "<img src=\"images/MPI_Gather.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of scatter and gather in our code. On each of the available processes, it will calculate an estimate of pi by drawing N random numbers. The master processes will assemble all of the estimates produced by all workers, and compute the mean and standard deviation across the independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_in_parallel(comm, N):\n",
    "\n",
    "    if rank == 0:\n",
    "        data = [N for i in range(size)]\n",
    "    else:\n",
    "        data = None\n",
    "    data = comm.scatter(data, root=0)\n",
    "    #\n",
    "    pi_est = estimate_pi(N)\n",
    "    #\n",
    "    pi_estimates = comm.gather(pi_est, root=0)\n",
    "    if rank == 0:\n",
    "        return pi_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to the Supercomputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mpi_montecarlo.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our Monte Carlo estimation in parallel on Stampede2\n",
    "\n",
    "Now, we will transfer the `pi_parallel.py` python file and the `job.sh` job script file to Stampede2. We want to run this job several times with different size core count requests to see how that affects how long the job takes. We will edit the job script to request different numbers of cores: 8, 16, 32, 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this program on Stampede2, we have to go through certain steps;\n",
    "\n",
    "1. Log into Stampede2 \n",
    "2. Get files from github repository\n",
    "3. Update `job.sh` file and submit jobs to batch\n",
    "4. Once a job completes, check output for successful completion\n",
    "5. Transfer output back to your computer, using Globus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Log into Stampede2 \n",
    "    - ssh to login.xsede.org then gsissh to Stampede2\n",
    "    \n",
    "    \n",
    "2. Get files from github repository    \n",
    "    - `git clone https://github.com/jeaimehp/directstem.git`\n",
    "    - This will create a directory \"directstem\" and put the files we need there\n",
    "    - `cd directstem`\n",
    "  \n",
    "  \n",
    "3. Update `job.sh` batch script and submit jobs to batch\n",
    "      - Edit (use either nano or vi) job.sh to modify the email address to yours\n",
    "\n",
    "        #SBATCH --mail-user=youremail@xxxx.edu\n",
    "        \n",
    "    - Save file (vi :w   nano Ctrl+o)\n",
    "    - Edit MPI tasks\n",
    "    \n",
    "        #SBATCH -n 64              # Total # of mpi tasks\n",
    "        \n",
    "    - If using vi, press i to allow editing and change 64 to 8, hit `esc`, then type `:w job8.sh`. This will save a new file. Repeat these steps for 16 and 32. When done type `:q!` to leave vi\n",
    "    - If using nano, edit line above, change 64 to 8. Ctrl+o change filename to `job8.sh`. Repeat these steps for 16 and 32. When done type Ctrl+x to leave nano, answer no.\n",
    "    Now you should have 4 job scripts: job.sh, job8.sh, job16.sh, job32.sh\n",
    "    - `sbatch  job.sh ` #submit each job\n",
    "        When you submit the job, you should see some statements checking that you have provided the correct information. The final statement should be \"Submitted batch job XXXXXXX\" That number is your jobid.\n",
    "    - squeue -u \\`whoami\\`  #check job status\n",
    "    \n",
    "    - `ls -alt` # list the contents of your directory in time order so you can see the newest files on the top\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Once job completes, check output\n",
    "    - `cat myjob.o*` or `cat myjob.oJobID#` #displays the contents of the job output file, fill in with the job id\n",
    "\n",
    "\n",
    "You should see a file like the one below. If you see any other messages or errors listed, then your job had a problem and didn't complete correctly. Please let the instructor know and we'll discuss it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat myjob.o5568792\n",
    "\n",
    "\n",
    "Currently Loaded Modules:\n",
    "  1) intel/18.0.2      4) git/2.24.1       7) cmake/3.16.1\n",
    "  2) libfabric/1.7.0   5) autotools/1.1    8) xalt/2.8\n",
    "  3) impi/18.0.2       6) python2/2.7.15   9) TACC\n",
    "\n",
    " \n",
    "\n",
    "/home1/04135/tg834346/ds-python\n",
    "Mon Apr 20 16:15:36 CDT 2020\n",
    "TACC:  Starting up job 5568792 \n",
    "TACC:  Starting parallel tasks... \n",
    "MPI size = 64\n",
    "1024 3.13482666015625 0.05217984714475068\n",
    "4096 3.1430206298828125 0.023611648596208255\n",
    "16384 3.1415481567382812 0.012168018465721247\n",
    "65536 3.1425952911376953 0.006008509973199845\n",
    "262144 3.141845464706421 0.0029074806908967335\n",
    "1048576 3.1416112184524536 0.0016187660049334007\n",
    "4194304 3.1415042132139206 0.0007705360257607911\n",
    "16777216 3.141533490270376 0.00037337832673420073\n",
    "67108864 3.1416473127901554 0.00019640405597756922\n",
    "268435456 3.1415855595842004 9.315265178432316e-05\n",
    "TACC:  Shutdown complete. Exiting. \n",
    "\n",
    "real\t1m58.948s\n",
    "user\t84m36.034s\n",
    "sys\t39m11.914s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at each of these output files to see how long the calculation took with different size core requests.\n",
    "\n",
    "Use the `grep` command to view the number of cores and time of each job\n",
    "\n",
    "`grep -E \"MPI\\real\" myjob.o*`\n",
    "\n",
    "| Number of Cores | Time   |\n",
    "|----------------|---------|\n",
    "| 8  |    12min 2.164sec    |\n",
    "| 16 |    6min 6.808sec    |\n",
    "| 32 |    3min 8.468sec    |\n",
    "| 64 |    2min 0.648sec    |\n",
    "\n",
    "\n",
    "Here is the \"real\" time data for one set of jobs with different core requests. You can see that generally the job gets significantly shorter as you increase the number of cores. The difference between 32 and 64 cores is not as large as you might expect. There is a limit to how parallel code can go, it isn't always effective to just add more cores. We might expect to see a drop off in speed if we ran this job with more than 64 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.pi)\n",
    "3.141592653589793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Transfer output back to your computer, using Globus\n",
    "    - Once you get the results for the 64 core job, we are going to transfer that file (`pi_output_64.csv`) back to your laptop.\n",
    "    - Use the Globus directions below to transfer the job output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer files to Stampede2\n",
    "    - Go to globus.org and “Log In”\n",
    "    - Go to “Your Collections” and choose your “Endpoint”\n",
    "    - Select the “Panel” with two(2) columns\n",
    "    - Add the Collection “XSEDE TACC Stampede 2”\n",
    "    - Select the file(s) you want to transfer and click “Start”\n",
    "    - Click “refresh list” to see the new files when completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Output\n",
    "\n",
    "The final activity is to visualize the statistics collected in the 64 node job. The code you can use to do this is below. Once, you've transferred the output from Stampede2 to your laptop, you can run this in Spyder. Make sure to change the path to the csv file or Spyder won't be able to find it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "output = pd.read_csv(\"/path to your file/pi_output.csv\", header=None)\n",
    "estimates=np.array(output)\n",
    "print(estimates)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(np.log2(estimates[:,0]), estimates[:,1], yerr=estimates[:,2])\n",
    "plt.ylabel('estimate of pi')\n",
    "plt.xlabel('log2(number of darts N)')\n",
    "plt.savefig('pi_vs_log2_N.png')\n",
    "plt.figure()\n",
    "plt.ylabel('log2(standard deviation)')\n",
    "plt.xlabel('log2(number of darts N)')\n",
    "plt.plot(np.log2(estimates[:,0]), np.log2(estimates[:,2]))\n",
    "plt.savefig('log2_std_vs_log2_N.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/estimate1.png\" width=\"600\"/>\n",
    "\n",
    "<img src=\"images/estimate1.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Here are some references for these exercises\n",
    "\n",
    "https://cvw.cac.cornell.edu/python/exercise\n",
    "\n",
    "https://rabernat.github.io/research_computing/parallel-programming-with-mpi-for-python.html\n",
    "\n",
    "https://cvw.cac.cornell.edu/MPI/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Python Reading\n",
    "\n",
    "Data Science: https://cvw.cac.cornell.edu/PyDataSci1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
